###########################
###################################################
# read in relevant SDI and location data files
###################################################
# SDI data downloaded from GBD
sdi.dat <- as.data.table(read_xlsx(path = paste0(local_data_dir, "/sdi/IHME_GBD_2019_SDI_1990_2019_Y2020M10D15.xlsx")))
# read in codebook for locations
loc.codebook <- as.data.table(read_xlsx(path = paste0(codebook_directory, "/IHME_GBD_2019_GBD_LOCATION_HIERARCHY_Y2020M10D15.xlsx")))
# locations that need to be added in manually
add.locations <- as.data.table(read_xlsx(path = paste0(codebook_directory, "/gbd_location_corrections.xlsx")))
###################################################
# Data Prep
###################################################
# re-name variable names of sdi data
names_sdi <- as.character(sdi.dat[1])
names(sdi.dat) <- names_sdi
sdi.dat <- sdi.dat[-c(1),]
# subset codebook to location ID and Name for matching
loc.cleaning <- loc.codebook[,.(`Location ID`, `Location Name`)]
# rename location name variable in all three data files
setnames(loc.cleaning, old=c("Location Name", "Location ID"), new=c("location", "id"))
setnames(sdi.dat, old=c("Location"), new=c("location"))
setnames(add.locations, old=c("Location ID", "Location Name"), new=c("id", "location"))
# create merge label name in both codebook and data
sdi.dat <- strip_chars(sdi.dat)
loc.cleaning <- strip_chars(loc.cleaning)
# bind rows with country names that are not spelled exactly the same in the codebook
loc.cleaning <- rbind(loc.cleaning, add.locations, fill=TRUE)
# check to make sure that all locations in codebook are in the data
code_check <- paste0(loc.cleaning$location)
data_check <- paste0(sdi.dat$location)
unmapped_codes <- sdi.dat[!data_check%in%code_check]
if(nrow(unmapped_codes)>0){
print(unique(unmapped_codes[, c("location"), with= FALSE]))
# print(unique(unmapped_codes$file_name)) #For documentation in the comments above.
stop("You have locations in the data that aren't in the codebook!")
}
# merge the location IDs to the sdi data
sdi.dat <- merge(sdi.dat, loc.cleaning[,.(id, location)], by = "location")
View(sdi.dat)
names(loc.codebook)
names(sdi.data)
names(sdi.dat)
# merge rest of the codebook values to the SDI data
sdi.dat <- merge(sdi.dat, loc.codebook, by.x = "id", by.y = "Location ID", all.x = TRUE)
View(sdi.dat)
# check for duplicate rows
duplicated(sdi.dat)
# check for duplicate rows
duplicated(sdi.dat[,.(location, id)])
# check for duplicate rows
View(sdi.dat[duplicated(sdi.dat[,.(location, id)])])
# check for duplicate rows
print(sdi.dat[duplicated(sdi.dat[,.(location, id)])])
View(sdi.dat)
View(sdi.dat[location=="Georgia"])
View(sdi.dat[location=="Georgia"])
View(sdi.dat)
View(sdi.dat[location=="georgia"])
# manually correct Georgia values (where country and US subnational region are misclassified)
sdi.georgia <- sdi.dat[location=="georgia"]
# manually correct Mexico values (where country and Mexico subnational region are misclassified)
sdi.mexico <- sdi.dat[location=="mexico"]
View(sdi.mexico)
# manually correct South Asia values (which is Super-Region and region)
sdi.southasia <- sdi.dat[location=="southasia"]
# manually correct North Africa values (which is Super-Region and region)
sdi.northafrica <- sdi.dat[location=="northafrica"]
sdi.northafrica
# manually correct North Africa values (which is Super-Region and region)
sdi.northafrica <- sdi.dat[location=="northafricaandmiddleeast"]
sdi.northafrica
sdi.southasia
# subset columns of sdi data
sdi.dat.2 <- sdi.dat[ location != c('georgia', 'mexico', 'southasia','northafricaandmiddleeast')]
# subset columns of sdi data
sdi.dat.2 <- sdi.dat[ location != c('georgia')]
sdi.dat.2[location=="georgia"]
view(sdi.georgia)
View(sdi.georgia)
sdi.georgia <- sdi.georgia[-c(2,3),]
View(sdi.georgia)
View(sdi.mexio)
View(sdi.mexico)
sdi.mexico <- sdi.mexico[-c(2,3),]
View(sdi.mexico)
View(sdi.southasia)
sdi.southasia <- sdi.southasia[-c(2,3),]
View(sdi.northafrica)
sdi.northafrica <- sdi.northafrica[-c(2,3),]
sdi.dat.prepped <- sdi.dat[location != c('georgia') | location != c('southasia')]
View(sdi.dat.prepped)
View(sdi.dat.prepped[location=="southasia"])
sdi.dat.prepped <- sdi.dat[location != c('georgia') & location != c('southasia')]
View(sdi.dat.prepped[location=="southasia"])
View(sdi.dat.prepped[location=="georgia"])
# remove rows that have been cleaned elsewhere
sdi.dat.prepped <- sdi.dat[location != c('georgia') &
location != c('southasia') &
location != c('mexico') &
location !=c('northafricaandmiddleeast')]
# add in rows for newly cleaned data
rbind(sdi.dat.prepped, sdi.georgia, sdi.mexico, sdi.southasia, sdi.northafrica)
# add in rows for newly cleaned data
sdi.dat.prepped <- rbind(sdi.dat.prepped, sdi.georgia, sdi.mexico, sdi.southasia, sdi.northafrica)
nrow(sdi.dat.prepped)
# check again for duplicates
print(sdi.dat[duplicated(sdi.dat[,.(location, id)])])
# check again for duplicates
print(sdi.dat.prepped[duplicated(sdi.dat[,.(location, id)])])
# check again for duplicates
print(sdi.dat.prepped[duplicated(sdi.dat.prepped[,.(location, id)])])
names(sdi.dat.prepped)
??setcolorder
# reshape data
sdi.dat.prepped.long = melt(sdi.dat.prepped, id.vars = c("id", "location", "orig_location", "Location Set Version ID",
"Location Name", "Parent ID", "Level", "Sort Order"),
variable.name = "year")
View(sdi.dat.prepped.long)
names(sdi.dat.prepped.long)
# reshape data
sdi.dat.prepped.long = melt(sdi.dat.prepped, id.vars = c("id", "location", "orig_location", "Location Set Version ID",
"Location Name", "Parent ID", "Level", "Sort Order"),
variable.name = "year", value.name = "sdi")
# subset columns
sdi.dat.prepped.long <- sdi.dat.prepped.long[,.("Location Name", "id", "year", "Parent ID", "Level", "Sort Order", "sdi", "Locatoin Set Version ID")]
View(sdi.dat.prepped.long)
# subset columns
sdi.dat.prepped.long <- sdi.dat.prepped.long[,.(`Location Name`, id, year, `Parent ID`, Level, `Sort Order`, sdi, `Location Set Version ID`)]
name(sdi.dat.prepped.long)
names(sdi.dat.prepped.long)
# reshape data
sdi.dat.prepped.long = melt(sdi.dat.prepped, id.vars = c("id", "location", "orig_location", "Location Set Version ID",
"Location Name", "Parent ID", "Level", "Sort Order"),
variable.name = "year", value.name = "sdi")
names(sdi.dat.prepped.long)
# rename columns in newly prepped data frame
setnames(sdi.dat.prepped.long,
old = c("Location Set Version ID", "Location Name", "Parent ID", "Level", "Sort Order"),
new = c("location_set_version_id", "location_name", "parent_id", "level", "sort_order"))
names(sdi.dat.prepped.long)
# subset columns
sdi.dat.prepped.long <- sdi.dat.prepped.long[,.c(location_name, id, location_set_version_id, parent_id, level, sort_order, year, sdi)]
# subset columns
sdi.dat.prepped.long <- sdi.dat.prepped.long[,.(location_name, id, location_set_version_id, parent_id, level, sort_order, year, sdi)]
View(sdi.dat.prepped.long)
# clean numerical values of SDIs
sdi.dat.prepped.long$sdi <- gsub("Â·",".",sdi.dat.prepped.long$sdi)
View(sdi.dat.prepped.long)
?saveRDS
prepped_data_dir <- "G:/My Drive/PHI/local_data/prepped_data/"
# save in prepped data folder
saveRDS(sdi.dat.prepped.long, file = paste0(prepped_data_dir, "sdi.RDS"))
rm(list=ls())
# Author: Francisco Rios
# Purpose: Prep Vaccination coverage data for analyses
# Date: Last modified July 13, 2021
####### read in vaccine trend data
# Read in list of files to prep
file_list <- data.table(read_excel(paste0(local_data_dir, "/data_file_list.xlsx")))
# subset files to vaccine trends
file_list <- file_list[data_type=="vaccination_trends"]
for(i in 1:nrow(file_list)){
# Set up file path
file_dir = paste0(local_data_dir, '/',file_list$data_type[i], '/', file_list$data_source[i], '/' )
# set up arguments
args <- list(file_dir, file_list$file_name[i], file_list$sheet[i], file_list$data_type[i])
### RUN THE PREP FUNCTION HERE ###
tmpData = do.call(prep_vax_trend_data, args)
#Add indexing data
append_cols = file_list[i, .(file_name, data_type, data_source, disease)]
stopifnot(nrow(append_cols)==1)
tmpData = cbind(tmpData, append_cols)
#Bind data together
if(i==1){
prepped_vax_data = tmpData
} else {
prepped_vax_data = rbind(prepped_vax_data, tmpData, use.names=TRUE, fill = TRUE)
}
print(paste0(i, " ", file_list$data_type[i], " ", file_list$disease[i], " ", file_list$file_name[i])) ## if the code breaks, you know which file it broke on
}
rm(list = )
rm(list=ls())
# Author: Francisco Rios
# Purpose: Set up R for prepping UW PHI Vaccination Data
# Date: Last modified July 12, 2021
# Manually set the working directory
###############################
# load required packages
###############################
library(data.table)
library(ggplot2)
library(readxl)
###############################
# important variables
###############################
shared_data_dir <- "G:/.shortcut-targets-by-id/1P7ITMVB9x01fuYfHW8-uWogw4SpbuvwO/Merck Vaccine Improvement Index Project/Data"
local_data_dir <- "G:/My Drive/PHI/local_data"
prepped_data_dir <- "G:/My Drive/PHI/local_data/prepped_data/"
code_dir <- "./"
codebook_directory <- paste0(local_data_dir,"/codebooks")
###############################
# source shared functions
###############################
source(paste0(code_dir, "functions/", "prep_vax_trend_data.R"))
source(paste0(code_dir, "functions/", "strip_chars.R"), encoding = "UTF-8")
###############################
# set Boolean switches
##############################
# Author: Francisco Rios
# Purpose: Prep Vaccination coverage data for analyses
# Date: Last modified July 13, 2021
####### read in vaccine trend data
# Read in list of files to prep
file_list <- data.table(read_excel(paste0(local_data_dir, "/data_file_list.xlsx")))
# subset files to vaccine trends
file_list <- file_list[data_type=="vaccination_trends"]
for(i in 1:nrow(file_list)){
# Set up file path
file_dir = paste0(local_data_dir, '/',file_list$data_type[i], '/', file_list$data_source[i], '/' )
# set up arguments
args <- list(file_dir, file_list$file_name[i], file_list$sheet[i], file_list$data_type[i])
### RUN THE PREP FUNCTION HERE ###
tmpData = do.call(prep_vax_trend_data, args)
#Add indexing data
append_cols = file_list[i, .(file_name, data_type, data_source, disease)]
stopifnot(nrow(append_cols)==1)
tmpData = cbind(tmpData, append_cols)
#Bind data together
if(i==1){
prepped_vax_data = tmpData
} else {
prepped_vax_data = rbind(prepped_vax_data, tmpData, use.names=TRUE, fill = TRUE)
}
print(paste0(i, " ", file_list$data_type[i], " ", file_list$disease[i], " ", file_list$file_name[i])) ## if the code breaks, you know which file it broke on
}
prepped_vax_data
View(prepped_vax_data)
# save prepped data
saveRDS(prepped_vax_data, paste0(prepped_data_dir, "vaccine_trends.RDS"))
outputFile2a = paste0(paste0(prepped_data_dir, "2a_vaccine_trends.RDS"))
outputFile2b = paste0(paste0(prepped_data_dir, "2b_sdi.RDS"))
# Read in the previously saved files for vaccination trends
vax_dt <- readRDS(outputFile2a)
# save prepped data
saveRDS(prepped_vax_data, paste0(prepped_data_dir, outputFile2a))
# save prepped data
saveRDS(prepped_vax_data, outputFile2a)
# Read in the previously saved files for vaccination trends
vax_dt <- readRDS(outputFile2a)
# read in the previously saved files for sdi
sdi_dt <- readRDS(outputFile2b) # this datatable contains all of the data
# Author: Francisco Rios
# Purpose: Prep socio-demographic data for analyses
# Date: Last modified July 13, 2021
########### To-dos:########
# need to find the loction ID for India (subnational estimates) somewhere
###########################
###################################################
# read in relevant SDI and location data files
###################################################
# SDI data downloaded from GBD
sdi.dat <- as.data.table(read_xlsx(path = paste0(local_data_dir, "/sdi/IHME_GBD_2019_SDI_1990_2019_Y2020M10D15.xlsx")))
# read in codebook for locations
loc.codebook <- as.data.table(read_xlsx(path = paste0(codebook_directory, "/IHME_GBD_2019_GBD_LOCATION_HIERARCHY_Y2020M10D15.xlsx")))
# locations that need to be added in manually
add.locations <- as.data.table(read_xlsx(path = paste0(codebook_directory, "/gbd_location_corrections.xlsx")))
###################################################
# Data Prep
###################################################
# re-name variable names of sdi data
names_sdi <- as.character(sdi.dat[1])
names(sdi.dat) <- names_sdi
sdi.dat <- sdi.dat[-c(1),]
# subset codebook to location ID and Name for matching
loc.cleaning <- loc.codebook[,.(`Location ID`, `Location Name`)]
# rename location name variable in all three data files
setnames(loc.cleaning, old=c("Location Name", "Location ID"), new=c("location", "id"))
setnames(sdi.dat, old=c("Location"), new=c("location"))
setnames(add.locations, old=c("Location ID", "Location Name"), new=c("id", "location"))
# create merge label name in both codebook and data
sdi.dat <- strip_chars(sdi.dat)
loc.cleaning <- strip_chars(loc.cleaning)
# bind rows with country names that are not spelled exactly the same in the codebook
loc.cleaning <- rbind(loc.cleaning, add.locations, fill=TRUE)
# check to make sure that all locations in codebook are in the data
code_check <- paste0(loc.cleaning$location)
data_check <- paste0(sdi.dat$location)
unmapped_codes <- sdi.dat[!data_check%in%code_check]
if(nrow(unmapped_codes)>0){
print(unique(unmapped_codes[, c("location"), with= FALSE]))
# print(unique(unmapped_codes$file_name)) #For documentation in the comments above.
stop("You have locations in the data that aren't in the codebook!")
}
# merge the location IDs to the sdi data
sdi.dat <- merge(sdi.dat, loc.cleaning[,.(id, location)], by = "location", all.x = TRUE)
# merge rest of the codebook values to the SDI data
sdi.dat <- merge(sdi.dat, loc.codebook, by.x = "id", by.y = "Location ID", all.x = TRUE)
# check for duplicate rows
print(sdi.dat[duplicated(sdi.dat[,.(location, id)])])
################## manual correction to prepped SDI data due to duplicate names
# manually correct Georgia values (where country and US subnational region are misclassified)
sdi.georgia <- sdi.dat[location=="georgia"]
sdi.georgia <- sdi.georgia[-c(2,3),]
# manually correct Mexico values (where country and Mexico subnational region are misclassified)
sdi.mexico <- sdi.dat[location=="mexico"]
sdi.mexico <- sdi.mexico[-c(2,3),]
# manually correct South Asia values (which is Super-Region and region)
sdi.southasia <- sdi.dat[location=="southasia"]
sdi.southasia <- sdi.southasia[-c(2,3),]
# manually correct North Africa values (which is Super-Region and region)
sdi.northafrica <- sdi.dat[location=="northafricaandmiddleeast"]
sdi.northafrica <- sdi.northafrica[-c(2,3),]
# remove rows that have been cleaned elsewhere
sdi.dat.prepped <- sdi.dat[location != c('georgia') &
location != c('southasia') &
location != c('mexico') &
location !=c('northafricaandmiddleeast')]
# add in rows for newly cleaned data
sdi.dat.prepped <- rbind(sdi.dat.prepped, sdi.georgia, sdi.mexico, sdi.southasia, sdi.northafrica)
# check again for duplicates
print(sdi.dat.prepped[duplicated(sdi.dat.prepped[,.(location, id)])])
# reshape data
sdi.dat.prepped.long = melt(sdi.dat.prepped, id.vars = c("id", "location", "orig_location", "Location Set Version ID",
"Location Name", "Parent ID", "Level", "Sort Order"),
variable.name = "year", value.name = "sdi")
# rename columns in newly prepped data frame
setnames(sdi.dat.prepped.long,
old = c("Location Set Version ID", "Location Name", "Parent ID", "Level", "Sort Order"),
new = c("location_set_version_id", "location_name", "parent_id", "level", "sort_order"))
# subset columns
sdi.dat.prepped.long <- sdi.dat.prepped.long[,.(location_name, id, location_set_version_id, parent_id, level, sort_order, year, sdi)]
# clean numerical values of SDIs
sdi.dat.prepped.long$sdi <- gsub("Â·",".",sdi.dat.prepped.long$sdi)
# save in prepped data folder
saveRDS(sdi.dat.prepped.long, outputFile2b)
# read in the previously saved files for sdi
sdi_dt <- readRDS(outputFile2b) # this datatable contains all of the data
names(vax_data)
names(vax_dt)
names(sdi_dt)
names(sdi.dat.prepped.long)
rm(list=ls())
# Author: Francisco Rios
# Purpose: Set up R for prepping UW PHI Vaccination Data
# Date: Last modified July 12, 2021
# Manually set the working directory
###############################
# load required packages
###############################
library(data.table)
library(ggplot2)
library(readxl)
###############################
# important variables
###############################
shared_data_dir <- "G:/.shortcut-targets-by-id/1P7ITMVB9x01fuYfHW8-uWogw4SpbuvwO/Merck Vaccine Improvement Index Project/Data"
local_data_dir <- "G:/My Drive/PHI/local_data"
prepped_data_dir <- "G:/My Drive/PHI/local_data/prepped_data/"
code_dir <- "./"
codebook_directory <- paste0(local_data_dir,"/codebooks")
###############################
# output files
###############################
outputFile2a = paste0(paste0(prepped_data_dir, "2a_vaccine_trends.RDS"))
outputFile2b = paste0(paste0(prepped_data_dir, "2b_sdi.RDS"))
###############################
# source shared functions
###############################
source(paste0(code_dir, "functions/", "prep_vax_trend_data.R"))
source(paste0(code_dir, "functions/", "strip_chars.R"), encoding = "UTF-8")
# Author: Francisco Rios
# Purpose: Prep Vaccination coverage data for analyses
# Date: Last modified July 13, 2021
####### read in vaccine trend data
# Read in list of files to prep
file_list <- data.table(read_excel(paste0(local_data_dir, "/data_file_list.xlsx")))
# subset files to vaccine trends
file_list <- file_list[data_type=="vaccination_trends"]
for(i in 1:nrow(file_list)){
# Set up file path
file_dir = paste0(local_data_dir, '/',file_list$data_type[i], '/', file_list$data_source[i], '/' )
# set up arguments
args <- list(file_dir, file_list$file_name[i], file_list$sheet[i], file_list$data_type[i])
### RUN THE PREP FUNCTION HERE ###
tmpData = do.call(prep_vax_trend_data, args)
#Add indexing data
append_cols = file_list[i, .(file_name, data_type, data_source, disease)]
stopifnot(nrow(append_cols)==1)
tmpData = cbind(tmpData, append_cols)
#Bind data together
if(i==1){
prepped_vax_data = tmpData
} else {
prepped_vax_data = rbind(prepped_vax_data, tmpData, use.names=TRUE, fill = TRUE)
}
print(paste0(i, " ", file_list$data_type[i], " ", file_list$disease[i], " ", file_list$file_name[i])) ## if the code breaks, you know which file it broke on
}
# formatting of data
#--if necessary--
# save prepped data
saveRDS(prepped_vax_data, outputFile2a)
# Author: Francisco Rios
# Purpose: Prep socio-demographic data for analyses
# Date: Last modified July 13, 2021
########### To-dos:########
# need to find the loction ID for India (subnational estimates) somewhere
###########################
###################################################
# read in relevant SDI and location data files
###################################################
# SDI data downloaded from GBD
sdi.dat <- as.data.table(read_xlsx(path = paste0(local_data_dir, "/sdi/IHME_GBD_2019_SDI_1990_2019_Y2020M10D15.xlsx")))
# read in codebook for locations
loc.codebook <- as.data.table(read_xlsx(path = paste0(codebook_directory, "/IHME_GBD_2019_GBD_LOCATION_HIERARCHY_Y2020M10D15.xlsx")))
# locations that need to be added in manually
add.locations <- as.data.table(read_xlsx(path = paste0(codebook_directory, "/gbd_location_corrections.xlsx")))
###################################################
# Data Prep
###################################################
# re-name variable names of sdi data
names_sdi <- as.character(sdi.dat[1])
names(sdi.dat) <- names_sdi
sdi.dat <- sdi.dat[-c(1),]
# subset codebook to location ID and Name for matching
loc.cleaning <- loc.codebook[,.(`Location ID`, `Location Name`)]
# rename location name variable in all three data files
setnames(loc.cleaning, old=c("Location Name", "Location ID"), new=c("location", "id"))
setnames(sdi.dat, old=c("Location"), new=c("location"))
setnames(add.locations, old=c("Location ID", "Location Name"), new=c("id", "location"))
# create merge label name in both codebook and data
sdi.dat <- strip_chars(sdi.dat)
loc.cleaning <- strip_chars(loc.cleaning)
# bind rows with country names that are not spelled exactly the same in the codebook
loc.cleaning <- rbind(loc.cleaning, add.locations, fill=TRUE)
# check to make sure that all locations in codebook are in the data
code_check <- paste0(loc.cleaning$location)
data_check <- paste0(sdi.dat$location)
unmapped_codes <- sdi.dat[!data_check%in%code_check]
if(nrow(unmapped_codes)>0){
print(unique(unmapped_codes[, c("location"), with= FALSE]))
# print(unique(unmapped_codes$file_name)) #For documentation in the comments above.
stop("You have locations in the data that aren't in the codebook!")
}
# merge the location IDs to the sdi data
sdi.dat <- merge(sdi.dat, loc.cleaning[,.(id, location)], by = "location", all.x = TRUE)
# merge rest of the codebook values to the SDI data
sdi.dat <- merge(sdi.dat, loc.codebook, by.x = "id", by.y = "Location ID", all.x = TRUE)
# check for duplicate rows
print(sdi.dat[duplicated(sdi.dat[,.(location, id)])])
################## manual correction to prepped SDI data due to duplicate names
# manually correct Georgia values (where country and US subnational region are misclassified)
sdi.georgia <- sdi.dat[location=="georgia"]
sdi.georgia <- sdi.georgia[-c(2,3),]
# manually correct Mexico values (where country and Mexico subnational region are misclassified)
sdi.mexico <- sdi.dat[location=="mexico"]
sdi.mexico <- sdi.mexico[-c(2,3),]
# manually correct South Asia values (which is Super-Region and region)
sdi.southasia <- sdi.dat[location=="southasia"]
sdi.southasia <- sdi.southasia[-c(2,3),]
# manually correct North Africa values (which is Super-Region and region)
sdi.northafrica <- sdi.dat[location=="northafricaandmiddleeast"]
sdi.northafrica <- sdi.northafrica[-c(2,3),]
# remove rows that have been cleaned elsewhere
sdi.dat.prepped <- sdi.dat[location != c('georgia') &
location != c('southasia') &
location != c('mexico') &
location !=c('northafricaandmiddleeast')]
# add in rows for newly cleaned data
sdi.dat.prepped <- rbind(sdi.dat.prepped, sdi.georgia, sdi.mexico, sdi.southasia, sdi.northafrica)
# check again for duplicates
print(sdi.dat.prepped[duplicated(sdi.dat.prepped[,.(location, id)])])
# reshape data
sdi.dat.prepped.long = melt(sdi.dat.prepped, id.vars = c("id", "location", "orig_location", "Location Set Version ID",
"Location Name", "Parent ID", "Level", "Sort Order"),
variable.name = "year", value.name = "sdi")
# rename columns in newly prepped data frame
setnames(sdi.dat.prepped.long,
old = c("Location Set Version ID", "Location Name", "Parent ID", "Level", "Sort Order", "id", "year"),
new = c("location_set_version_id", "location_name", "parent_id", "level", "sort_order", "location_id", "year_id" ))
# subset columns
sdi.dat.prepped.long <- sdi.dat.prepped.long[,.(location_name, id, location_set_version_id, parent_id, level, sort_order, year, sdi)]
# clean numerical values of SDIs
sdi.dat.prepped.long$sdi <- gsub("Â·",".",sdi.dat.prepped.long$sdi)
# save in prepped data folder
saveRDS(sdi.dat.prepped.long, outputFile2b)
# Author: Francisco Rios
# Purpose: Merges all data to be used in analyses
# Date: Last modified July 13, 2021
# Read in the previously saved files for vaccination trends
vax_dt <- readRDS(outputFile2a)
# read in the previously saved files for sdi
sdi_dt <- readRDS(outputFile2b) # this datatable contains all of the data
# merge two data tables sheets together
dt <- merge(vax_dt, sdi_dt, by=c("location_name", "location_id", "year_id"), all.x = TRUE)
names(sdi_dt)
names(vax_dt)
str(vax_dt)
str(sdi_dt)
View(sdi.dat.prepped.long)
# convert variable structures
as.numeric(levels(sdi.dat.prepped.long$year_id))[sdi.dat.prepped.long$year_id]
# convert variable structures
sdi.dat.prepped.long(as.numeric(levels(sdi.dat.prepped.long$year_id))[sdi.dat.prepped.long$year_id])
# convert variable structures
sdi.dat.prepped.long$year_id <- as.numeric(levels(sdi.dat.prepped.long$year_id))[sdi.dat.prepped.long$year_id])
# convert variable structures
sdi.dat.prepped.long$year_id <- as.numeric(levels(sdi.dat.prepped.long$year_id))[sdi.dat.prepped.long$year_id]
str(sdi.dat.prepped.long)
sdi.dat.prepped.long$sdi <- as.numeric(levels(sdi.dat.prepped.long$sdi))[sdi.dat.prepped.long$sdi]
str(sdi.dat.prepped.long)
?glm
