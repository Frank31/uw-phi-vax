prep_sdi = TRUE
prep_vax_trends = TRUE
# Author: Francisco Rios
# Purpose: Prep socio-demographic data for analyses
# Date: Last modified July 13, 2021
########### To-dos:########
# need to find the loction ID for India (subnational estimates) somewhere
# add check to make sure there are no duplicate rows added or removed
###########################
###################################################
# read in relevant SDI and location data files
###################################################
# SDI data downloaded from GBD
sdi.dat <- as.data.table(read_xlsx(path = paste0(local_data_dir, "sdi/IHME_GBD_2019_SDI_1990_2019_Y2020M10D15.xlsx")))
# read in codebook for locations
loc.codebook <- as.data.table(read_xlsx(path = paste0(codebook_directory, "IHME_GBD_2019_GBD_LOCATION_HIERARCHY_Y2020M10D15.xlsx")))
# locations that need to be added in manually
add.locations <- as.data.table(read_xlsx(path = paste0(codebook_directory, "gbd_location_corrections.xlsx")))
###################################################
# Data Prep
###################################################
# re-name variable names of sdi data
names_sdi <- as.character(sdi.dat[1])
names(sdi.dat) <- names_sdi
sdi.dat <- sdi.dat[-c(1),]
# subset codebook to location ID and Name for matching
loc.cleaning <- loc.codebook[,.(`Location ID`, `Location Name`)]
# rename location name variable in all three data files
setnames(loc.cleaning, old=c("Location Name", "Location ID"), new=c("location", "id"))
setnames(sdi.dat, old=c("Location"), new=c("location"))
setnames(add.locations, old=c("Location ID", "Location Name"), new=c("id", "location"))
# bind rows with country names that are not spelled exactly the same in the codebook
loc.cleaning <- rbind(loc.cleaning, add.locations, fill=TRUE)
# create merge label name in both codebook and data
sdi.dat <- strip_chars(sdi.dat)
loc.cleaning <- strip_chars(loc.cleaning)
# check to make sure that all locations in codebook are in the data
code_check <- paste0(loc.cleaning$location)
data_check <- paste0(sdi.dat$location)
unmapped_codes <- sdi.dat[!data_check%in%code_check]
if(nrow(unmapped_codes)>0){
print(unique(unmapped_codes[, c("location"), with= FALSE]))
# print(unique(unmapped_codes$file_name)) #For documentation in the comments above.
stop("You have locations in the data that aren't in the codebook!")
}
#############################################
# remove unmapped locations without a location ID for now
#############################################
sdi.dat <- sdi.dat[!location%in%unmapped_codes$location]
# merge the location IDs to the sdi data
sdi.dat <- merge(sdi.dat, loc.cleaning[,.(id, location)], by = "location", all.x = TRUE)
# merge rest of the codebook values to the SDI data
sdi.dat <- merge(sdi.dat, loc.codebook, by.x = "id", by.y = "Location ID", all.x = TRUE)
# check for duplicate rows
print(sdi.dat[duplicated(sdi.dat[,.(location, id)])])
################## manual correction to prepped SDI data due to duplicate names
# manually correct Georgia values (where country and US subnational region are misclassified)
sdi.georgia <- sdi.dat[location=="georgia"]
sdi.georgia <- sdi.georgia[-c(2,3),]
# manually correct Mexico values (where country and Mexico subnational region are misclassified)
sdi.mexico <- sdi.dat[location=="mexico"]
sdi.mexico <- sdi.mexico[-c(2,3),]
# manually correct South Asia values (which is Super-Region and region)
sdi.southasia <- sdi.dat[location=="southasia"]
sdi.southasia <- sdi.southasia[-c(2,3),]
# manually correct North Africa values (which is Super-Region and region)
sdi.northafrica <- sdi.dat[location=="northafricaandmiddleeast"]
sdi.northafrica <- sdi.northafrica[-c(2,3),]
# remove rows that have been cleaned elsewhere
sdi.dat.prepped <- sdi.dat[location != c('georgia') &
location != c('southasia') &
location != c('mexico') &
location !=c('northafricaandmiddleeast')]
# add in rows for newly cleaned data
sdi.dat.prepped <- rbind(sdi.dat.prepped, sdi.georgia, sdi.mexico, sdi.southasia, sdi.northafrica)
# check again for duplicates
print(sdi.dat.prepped[duplicated(sdi.dat.prepped[,.(location, id)])])
names(sdi.dat.prepped)
# rename columns in data frame
setnames(sdi.dat.prepped,
old = c("Location Set Version ID", "Location Name", "Parent ID", "Level", "Sort Order", "id", "year"),
new = c("location_set_version_id", "location_name", "parent_id", "level", "sort_order", "location_id", "year_id" ))
# rename columns in data frame
setnames(sdi.dat.prepped,
old = c("Location Set Version ID", "Location Name", "Parent ID", "Level", "Sort Order", "id"),
new = c("location_set_version_id", "location_name", "parent_id", "level", "sort_order", "location_id"))
names(sdi.dat.prepped)
# subset columns
sdi.dat.prepped <- sdi.dat.prepped[,.(location_name, location_id, location_set_version_id, parent_id, level, sort_order, sdi)]
# subset columns
sdi.dat.prepped <- sdi.dat.prepped[,.(location_name, location_id, location_set_version_id, parent_id, level, sort_order)]
View(sdi.dat.prepped)
rm(list=ls())
# Author: Francisco Rios
# Purpose: Set up R for prepping UW PHI Vaccination Data
# Date: Last modified July 12, 2021
# Manually set the working directory
###############################
# load required packages
###############################
library(data.table)
library(ggplot2)
library(readxl)
library(GGally)
###############################
# important variables
###############################
shared_data_dir <- "G:/.shortcut-targets-by-id/1P7ITMVB9x01fuYfHW8-uWogw4SpbuvwO/Merck Vaccine Improvement Index Project/Data/"
local_data_dir <- "G:/My Drive/PHI/local_data/"
prepped_data_dir <- "G:/My Drive/PHI/local_data/prepped_data/"
code_dir <- "./"
codebook_directory <- paste0(local_data_dir,"/codebooks/")
visDir <- paste0("G:/My Drive/PHI/visualizations/")
###############################
# output files
###############################
outputFile2a = paste0(prepped_data_dir, "2a_vaccine_trends.RDS")
outputFile2b = paste0(prepped_data_dir, "2b_sdi.RDS")
outputFile3 = paste0(prepped_data_dir, "3a_merged_data.RDS")
outputFile4 = paste0(visDir, "aim1_sample_visualizations.PDF")
outputFile4b = paste0(visDir, "aim1_sample_country_trends.PDF")
###############################
# source shared functions
###############################
source(paste0(code_dir, "functions/", "prep_vax_trend_data.R"))
source(paste0(code_dir, "functions/", "strip_chars.R"), encoding = "UTF-8")
###############################
# set Boolean switches
##############################
prep_sdi = TRUE
prep_vax_trends = TRUE
# Author: Francisco Rios
# Purpose: Prep socio-demographic data for analyses
# Date: Last modified July 13, 2021
########### To-dos:########
# need to find the loction ID for India (subnational estimates) somewhere
# add check to make sure there are no duplicate rows added or removed
###########################
###################################################
# read in relevant SDI and location data files
###################################################
# SDI data downloaded from GBD
sdi.dat <- as.data.table(read_xlsx(path = paste0(local_data_dir, "sdi/IHME_GBD_2019_SDI_1990_2019_Y2020M10D15.xlsx")))
# read in codebook for locations
loc.codebook <- as.data.table(read_xlsx(path = paste0(codebook_directory, "IHME_GBD_2019_GBD_LOCATION_HIERARCHY_Y2020M10D15.xlsx")))
# locations that need to be added in manually
add.locations <- as.data.table(read_xlsx(path = paste0(codebook_directory, "gbd_location_corrections.xlsx")))
###################################################
# Data Prep
###################################################
# re-name variable names of sdi data
names_sdi <- as.character(sdi.dat[1])
names(sdi.dat) <- names_sdi
sdi.dat <- sdi.dat[-c(1),]
# subset codebook to location ID and Name for matching
loc.cleaning <- loc.codebook[,.(`Location ID`, `Location Name`)]
# rename location name variable in all three data files
setnames(loc.cleaning, old=c("Location Name", "Location ID"), new=c("location", "id"))
setnames(sdi.dat, old=c("Location"), new=c("location"))
setnames(add.locations, old=c("Location ID", "Location Name"), new=c("id", "location"))
# bind rows with country names that are not spelled exactly the same in the codebook
loc.cleaning <- rbind(loc.cleaning, add.locations, fill=TRUE)
# create merge label name in both codebook and data
sdi.dat <- strip_chars(sdi.dat)
loc.cleaning <- strip_chars(loc.cleaning)
# check to make sure that all locations in codebook are in the data
code_check <- paste0(loc.cleaning$location)
data_check <- paste0(sdi.dat$location)
unmapped_codes <- sdi.dat[!data_check%in%code_check]
if(nrow(unmapped_codes)>0){
print(unique(unmapped_codes[, c("location"), with= FALSE]))
# print(unique(unmapped_codes$file_name)) #For documentation in the comments above.
stop("You have locations in the data that aren't in the codebook!")
}
#############################################
# remove unmapped locations without a location ID for now
#############################################
sdi.dat <- sdi.dat[!location%in%unmapped_codes$location]
# merge the location IDs to the sdi data
sdi.dat <- merge(sdi.dat, loc.cleaning[,.(id, location)], by = "location", all.x = TRUE)
# merge rest of the codebook values to the SDI data
sdi.dat <- merge(sdi.dat, loc.codebook, by.x = "id", by.y = "Location ID", all.x = TRUE)
# check for duplicate rows
print(sdi.dat[duplicated(sdi.dat[,.(location, id)])])
################## manual correction to prepped SDI data due to duplicate names
# manually correct Georgia values (where country and US subnational region are misclassified)
sdi.georgia <- sdi.dat[location=="georgia"]
sdi.georgia <- sdi.georgia[-c(2,3),]
# manually correct Mexico values (where country and Mexico subnational region are misclassified)
sdi.mexico <- sdi.dat[location=="mexico"]
sdi.mexico <- sdi.mexico[-c(2,3),]
# manually correct South Asia values (which is Super-Region and region)
sdi.southasia <- sdi.dat[location=="southasia"]
sdi.southasia <- sdi.southasia[-c(2,3),]
# manually correct North Africa values (which is Super-Region and region)
sdi.northafrica <- sdi.dat[location=="northafricaandmiddleeast"]
sdi.northafrica <- sdi.northafrica[-c(2,3),]
# remove rows that have been cleaned elsewhere
sdi.dat.prepped <- sdi.dat[location != c('georgia') &
location != c('southasia') &
location != c('mexico') &
location !=c('northafricaandmiddleeast')]
# add in rows for newly cleaned data
sdi.dat.prepped <- rbind(sdi.dat.prepped, sdi.georgia, sdi.mexico, sdi.southasia, sdi.northafrica)
# check again for duplicates
print(sdi.dat.prepped[duplicated(sdi.dat.prepped[,.(location, id)])])
# rename columns in data frame
setnames(sdi.dat.prepped,
old = c("Location Set Version ID", "Location Name", "Parent ID", "Level", "Sort Order", "id"),
new = c("location_set_version_id", "location_name", "parent_id", "level", "sort_order", "location_id"))
View(sdi.dat.prepped)
# clean numerical values of SDIs
apply(sdi.dat.prepped, 2, function(y) (gsub("·", ".", y)))
sdi.dat.prepped <- sdi.dat.prepped.long$sdi <- gsub("·",".",sdi.dat.prepped.long$sdi)
# clean numerical values of SDIs
sdi.dat.prepped <- apply(sdi.dat.prepped, 2, function(y) (gsub("·", ".", y)))
View(sdi.dat.prepped)
names(sdi.dat.prepped)
names(sdi.dat.prepped)
str(sdi.dat.prepped)
# Author: Francisco Rios
# Purpose: Prep socio-demographic data for analyses
# Date: Last modified July 13, 2021
########### To-dos:########
# need to find the loction ID for India (subnational estimates) somewhere
# add check to make sure there are no duplicate rows added or removed
###########################
###################################################
# read in relevant SDI and location data files
###################################################
# SDI data downloaded from GBD
sdi.dat <- as.data.table(read_xlsx(path = paste0(local_data_dir, "sdi/IHME_GBD_2019_SDI_1990_2019_Y2020M10D15.xlsx")))
# read in codebook for locations
loc.codebook <- as.data.table(read_xlsx(path = paste0(codebook_directory, "IHME_GBD_2019_GBD_LOCATION_HIERARCHY_Y2020M10D15.xlsx")))
# locations that need to be added in manually
add.locations <- as.data.table(read_xlsx(path = paste0(codebook_directory, "gbd_location_corrections.xlsx")))
###################################################
# Data Prep
###################################################
# re-name variable names of sdi data
names_sdi <- as.character(sdi.dat[1])
names(sdi.dat) <- names_sdi
sdi.dat <- sdi.dat[-c(1),]
# subset codebook to location ID and Name for matching
loc.cleaning <- loc.codebook[,.(`Location ID`, `Location Name`)]
# rename location name variable in all three data files
setnames(loc.cleaning, old=c("Location Name", "Location ID"), new=c("location", "id"))
setnames(sdi.dat, old=c("Location"), new=c("location"))
setnames(add.locations, old=c("Location ID", "Location Name"), new=c("id", "location"))
# bind rows with country names that are not spelled exactly the same in the codebook
loc.cleaning <- rbind(loc.cleaning, add.locations, fill=TRUE)
# create merge label name in both codebook and data
sdi.dat <- strip_chars(sdi.dat)
loc.cleaning <- strip_chars(loc.cleaning)
# check to make sure that all locations in codebook are in the data
code_check <- paste0(loc.cleaning$location)
data_check <- paste0(sdi.dat$location)
unmapped_codes <- sdi.dat[!data_check%in%code_check]
if(nrow(unmapped_codes)>0){
print(unique(unmapped_codes[, c("location"), with= FALSE]))
# print(unique(unmapped_codes$file_name)) #For documentation in the comments above.
stop("You have locations in the data that aren't in the codebook!")
}
#############################################
# remove unmapped locations without a location ID for now
#############################################
sdi.dat <- sdi.dat[!location%in%unmapped_codes$location]
# merge the location IDs to the sdi data
sdi.dat <- merge(sdi.dat, loc.cleaning[,.(id, location)], by = "location", all.x = TRUE)
# merge rest of the codebook values to the SDI data
sdi.dat <- merge(sdi.dat, loc.codebook, by.x = "id", by.y = "Location ID", all.x = TRUE)
# check for duplicate rows
print(sdi.dat[duplicated(sdi.dat[,.(location, id)])])
################## manual correction to prepped SDI data due to duplicate names
# manually correct Georgia values (where country and US subnational region are misclassified)
sdi.georgia <- sdi.dat[location=="georgia"]
sdi.georgia <- sdi.georgia[-c(2,3),]
# manually correct Mexico values (where country and Mexico subnational region are misclassified)
sdi.mexico <- sdi.dat[location=="mexico"]
sdi.mexico <- sdi.mexico[-c(2,3),]
# manually correct South Asia values (which is Super-Region and region)
sdi.southasia <- sdi.dat[location=="southasia"]
sdi.southasia <- sdi.southasia[-c(2,3),]
# manually correct North Africa values (which is Super-Region and region)
sdi.northafrica <- sdi.dat[location=="northafricaandmiddleeast"]
sdi.northafrica <- sdi.northafrica[-c(2,3),]
# remove rows that have been cleaned elsewhere
sdi.dat.prepped <- sdi.dat[location != c('georgia') &
location != c('southasia') &
location != c('mexico') &
location !=c('northafricaandmiddleeast')]
# add in rows for newly cleaned data
sdi.dat.prepped <- rbind(sdi.dat.prepped, sdi.georgia, sdi.mexico, sdi.southasia, sdi.northafrica)
# check again for duplicates
print(sdi.dat.prepped[duplicated(sdi.dat.prepped[,.(location, id)])])
# rename columns in data frame
setnames(sdi.dat.prepped,
old = c("Location Set Version ID", "Location Name", "Parent ID", "Level", "Sort Order", "id"),
new = c("location_set_version_id", "location_name", "parent_id", "level", "sort_order", "location_id"))
names(sdi.dat.prepped)
# clean numerical values of SDIs
cols = names(sdi.dat.prepped)[3:32]   # define which columns to work with
cols
sdi.dat.prepped[ , (cols) := lapply(.SD, function(x) {gsub("·", ".", x)}), .SDcols = cols] # replace symbol
View(sdi.dat.prepped)
names(sdi.dat.prepped)
head(sdi.dat.prepped)
# # remove unnecessary columns
sdi.dat.prepped[,c("orig_location", "location"):=NULL]
# Author: Francisco Rios
# Purpose: Prep socio-demographic data for analyses
# Date: Last modified July 13, 2021
########### To-dos:########
# need to find the loction ID for India (subnational estimates) somewhere
# add check to make sure there are no duplicate rows added or removed
###########################
###################################################
# read in relevant SDI and location data files
###################################################
# SDI data downloaded from GBD
sdi.dat <- as.data.table(read_xlsx(path = paste0(local_data_dir, "sdi/IHME_GBD_2019_SDI_1990_2019_Y2020M10D15.xlsx")))
# read in codebook for locations
loc.codebook <- as.data.table(read_xlsx(path = paste0(codebook_directory, "IHME_GBD_2019_GBD_LOCATION_HIERARCHY_Y2020M10D15.xlsx")))
# locations that need to be added in manually
add.locations <- as.data.table(read_xlsx(path = paste0(codebook_directory, "gbd_location_corrections.xlsx")))
###################################################
# Data Prep
###################################################
# re-name variable names of sdi data
names_sdi <- as.character(sdi.dat[1])
names(sdi.dat) <- names_sdi
sdi.dat <- sdi.dat[-c(1),]
# subset codebook to location ID and Name for matching
loc.cleaning <- loc.codebook[,.(`Location ID`, `Location Name`)]
# rename location name variable in all three data files
setnames(loc.cleaning, old=c("Location Name", "Location ID"), new=c("location", "id"))
setnames(sdi.dat, old=c("Location"), new=c("location"))
setnames(add.locations, old=c("Location ID", "Location Name"), new=c("id", "location"))
# bind rows with country names that are not spelled exactly the same in the codebook
loc.cleaning <- rbind(loc.cleaning, add.locations, fill=TRUE)
# create merge label name in both codebook and data
sdi.dat <- strip_chars(sdi.dat)
loc.cleaning <- strip_chars(loc.cleaning)
# check to make sure that all locations in codebook are in the data
code_check <- paste0(loc.cleaning$location)
data_check <- paste0(sdi.dat$location)
unmapped_codes <- sdi.dat[!data_check%in%code_check]
if(nrow(unmapped_codes)>0){
print(unique(unmapped_codes[, c("location"), with= FALSE]))
# print(unique(unmapped_codes$file_name)) #For documentation in the comments above.
stop("You have locations in the data that aren't in the codebook!")
}
#############################################
# remove unmapped locations without a location ID for now
#############################################
sdi.dat <- sdi.dat[!location%in%unmapped_codes$location]
# merge the location IDs to the sdi data
sdi.dat <- merge(sdi.dat, loc.cleaning[,.(id, location)], by = "location", all.x = TRUE)
# merge rest of the codebook values to the SDI data
sdi.dat <- merge(sdi.dat, loc.codebook, by.x = "id", by.y = "Location ID", all.x = TRUE)
# check for duplicate rows
print(sdi.dat[duplicated(sdi.dat[,.(location, id)])])
################## manual correction to prepped SDI data due to duplicate names
# manually correct Georgia values (where country and US subnational region are misclassified)
sdi.georgia <- sdi.dat[location=="georgia"]
sdi.georgia <- sdi.georgia[-c(2,3),]
# manually correct Mexico values (where country and Mexico subnational region are misclassified)
sdi.mexico <- sdi.dat[location=="mexico"]
sdi.mexico <- sdi.mexico[-c(2,3),]
# manually correct South Asia values (which is Super-Region and region)
sdi.southasia <- sdi.dat[location=="southasia"]
sdi.southasia <- sdi.southasia[-c(2,3),]
# manually correct North Africa values (which is Super-Region and region)
sdi.northafrica <- sdi.dat[location=="northafricaandmiddleeast"]
sdi.northafrica <- sdi.northafrica[-c(2,3),]
# remove rows that have been cleaned elsewhere
sdi.dat.prepped <- sdi.dat[location != c('georgia') &
location != c('southasia') &
location != c('mexico') &
location !=c('northafricaandmiddleeast')]
# add in rows for newly cleaned data
sdi.dat.prepped <- rbind(sdi.dat.prepped, sdi.georgia, sdi.mexico, sdi.southasia, sdi.northafrica)
# check again for duplicates
print(sdi.dat.prepped[duplicated(sdi.dat.prepped[,.(location, id)])])
# rename columns in data frame
setnames(sdi.dat.prepped,
old = c("Location Set Version ID", "Location Name", "Parent ID", "Level", "Sort Order", "id"),
new = c("location_set_version_id", "location_name", "parent_id", "level", "sort_order", "location_id"))
# clean numerical values of SDIs
cols = names(sdi.dat.prepped)[3:32]   # define which columns to work with
sdi.dat.prepped[ , (cols) := lapply(.SD, function(x) { as.numeric(gsub("·", ".", x)) }), .SDcols = cols] # replace symbol
str(sdi.dat.prepped)
# remove unnecessary columns
sdi.dat.prepped[,c("orig_location", "location"):=NULL]
names(sdi.dat.prepped)
# calculate tertiles from 2019 country-level data
sdi.ter <- sdi.dat.prepped[level==3]$`2019`
quantile(sdi.ter, c(0:3/3))
# classify countries into groups based on SDI in 2019
sdi.dat.prepped$sdi_group[sdi.dat.prepped$`2019` <= 0.5790 ] <- "low"
sdi.dat.prepped$sdi_group[sdi.dat.prepped$`2019` > 0.5790 & sdi.dat.prepped$`2019` <= 0.7423 ] <- "medium"
sdi.dat.prepped$sdi_group[sdi.dat.prepped$`2019` > 0.7423 ] <- "high"
View(sdi.dat.prepped)
names(sdi.dat.prepped)
# reshape data
sdi.dat.prepped.long = melt(sdi.dat.prepped, id.vars = c("location_name", "location_id", "location_set_version_id",
"sdi_group", "parent_id", "level", "sort_order"),
variable.name = "year", value.name = "sdi")
View(sdi.dat.prepped.long)
# convert variable structures
sdi.dat.prepped.long$year_id <- as.numeric(levels(sdi.dat.prepped.long$year_id))[sdi.dat.prepped.long$year_id]
# Author: Francisco Rios
# Purpose: Prep socio-demographic data for analyses
# Date: Last modified July 13, 2021
########### To-dos:########
# need to find the loction ID for India (subnational estimates) somewhere
# add check to make sure there are no duplicate rows added or removed
###########################
###################################################
# read in relevant SDI and location data files
###################################################
# SDI data downloaded from GBD
sdi.dat <- as.data.table(read_xlsx(path = paste0(local_data_dir, "sdi/IHME_GBD_2019_SDI_1990_2019_Y2020M10D15.xlsx")))
# read in codebook for locations
loc.codebook <- as.data.table(read_xlsx(path = paste0(codebook_directory, "IHME_GBD_2019_GBD_LOCATION_HIERARCHY_Y2020M10D15.xlsx")))
# locations that need to be added in manually
add.locations <- as.data.table(read_xlsx(path = paste0(codebook_directory, "gbd_location_corrections.xlsx")))
###################################################
# Data Prep
###################################################
# re-name variable names of sdi data
names_sdi <- as.character(sdi.dat[1])
names(sdi.dat) <- names_sdi
sdi.dat <- sdi.dat[-c(1),]
# subset codebook to location ID and Name for matching
loc.cleaning <- loc.codebook[,.(`Location ID`, `Location Name`)]
# rename location name variable in all three data files
setnames(loc.cleaning, old=c("Location Name", "Location ID"), new=c("location", "id"))
setnames(sdi.dat, old=c("Location"), new=c("location"))
setnames(add.locations, old=c("Location ID", "Location Name"), new=c("id", "location"))
# bind rows with country names that are not spelled exactly the same in the codebook
loc.cleaning <- rbind(loc.cleaning, add.locations, fill=TRUE)
# create merge label name in both codebook and data
sdi.dat <- strip_chars(sdi.dat)
loc.cleaning <- strip_chars(loc.cleaning)
# check to make sure that all locations in codebook are in the data
code_check <- paste0(loc.cleaning$location)
data_check <- paste0(sdi.dat$location)
unmapped_codes <- sdi.dat[!data_check%in%code_check]
if(nrow(unmapped_codes)>0){
print(unique(unmapped_codes[, c("location"), with= FALSE]))
# print(unique(unmapped_codes$file_name)) #For documentation in the comments above.
stop("You have locations in the data that aren't in the codebook!")
}
#############################################
# remove unmapped locations without a location ID for now
#############################################
sdi.dat <- sdi.dat[!location%in%unmapped_codes$location]
# merge the location IDs to the sdi data
sdi.dat <- merge(sdi.dat, loc.cleaning[,.(id, location)], by = "location", all.x = TRUE)
# merge rest of the codebook values to the SDI data
sdi.dat <- merge(sdi.dat, loc.codebook, by.x = "id", by.y = "Location ID", all.x = TRUE)
# check for duplicate rows
print(sdi.dat[duplicated(sdi.dat[,.(location, id)])])
################## manual correction to prepped SDI data due to duplicate names
# manually correct Georgia values (where country and US subnational region are misclassified)
sdi.georgia <- sdi.dat[location=="georgia"]
sdi.georgia <- sdi.georgia[-c(2,3),]
# manually correct Mexico values (where country and Mexico subnational region are misclassified)
sdi.mexico <- sdi.dat[location=="mexico"]
sdi.mexico <- sdi.mexico[-c(2,3),]
# manually correct South Asia values (which is Super-Region and region)
sdi.southasia <- sdi.dat[location=="southasia"]
sdi.southasia <- sdi.southasia[-c(2,3),]
# manually correct North Africa values (which is Super-Region and region)
sdi.northafrica <- sdi.dat[location=="northafricaandmiddleeast"]
sdi.northafrica <- sdi.northafrica[-c(2,3),]
# remove rows that have been cleaned elsewhere
sdi.dat.prepped <- sdi.dat[location != c('georgia') &
location != c('southasia') &
location != c('mexico') &
location !=c('northafricaandmiddleeast')]
# add in rows for newly cleaned data
sdi.dat.prepped <- rbind(sdi.dat.prepped, sdi.georgia, sdi.mexico, sdi.southasia, sdi.northafrica)
# check again for duplicates
print(sdi.dat.prepped[duplicated(sdi.dat.prepped[,.(location, id)])])
# rename columns in data frame
setnames(sdi.dat.prepped,
old = c("Location Set Version ID", "Location Name", "Parent ID", "Level", "Sort Order", "id"),
new = c("location_set_version_id", "location_name", "parent_id", "level", "sort_order", "location_id"))
# clean numerical values of SDIs
cols = names(sdi.dat.prepped)[3:32]   # define which columns to work with
sdi.dat.prepped[ , (cols) := lapply(.SD, function(x) { as.numeric(gsub("·", ".", x)) }), .SDcols = cols] # replace symbol
# remove unnecessary columns
sdi.dat.prepped[,c("orig_location", "location"):=NULL]
# calculate tertiles from 2019 country-level data
sdi.ter <- sdi.dat.prepped[level==3]$`2019`
# quantile(sdi.ter, c(0:3/3))
# classify countries into groups based on SDI in 2019
sdi.dat.prepped$sdi_group[sdi.dat.prepped$`2019` <= 0.5790 ] <- "low"
sdi.dat.prepped$sdi_group[sdi.dat.prepped$`2019` > 0.5790 & sdi.dat.prepped$`2019` <= 0.7423 ] <- "medium"
sdi.dat.prepped$sdi_group[sdi.dat.prepped$`2019` > 0.7423 ] <- "high"
# reshape data
sdi.dat.prepped.long = melt(sdi.dat.prepped, id.vars = c("location_name", "location_id", "location_set_version_id",
"sdi_group", "parent_id", "level", "sort_order"),
variable.name = "year_id", value.name = "sdi")
View(sdi.dat.prepped.long)
# convert variable structures
sdi.dat.prepped.long$year_id <- as.numeric(levels(sdi.dat.prepped.long$year_id))[sdi.dat.prepped.long$year_id]
# save in prepped data folder
saveRDS(sdi.dat.prepped.long, outputFile2b)
View(sdi.dat.prepped.long)
